<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="google机器学习速成课程总结"><meta name="keywords" content="Python,MachineLearning,TensorFlow"><meta name="author" content="Ykk,undefined"><meta name="copyright" content="Ykk"><title>google机器学习速成课程总结 | Ykk</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.4"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://unpkg.com"><link rel="stylesheet" type="text/css" href="https://unpkg.com/gitment@latest/style/default.css"><script src="https://unpkg.com/gitment@latest/dist/gitment.browser.js"></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#机器学习概念"><span class="toc-number">1.</span> <span class="toc-text">机器学习概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#机器学习主要术语"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习主要术语</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#标签"><span class="toc-number">1.1.1.</span> <span class="toc-text">标签</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特征"><span class="toc-number">1.1.2.</span> <span class="toc-text">特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#样本"><span class="toc-number">1.1.3.</span> <span class="toc-text">样本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#模型"><span class="toc-number">1.1.4.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#回归与分类"><span class="toc-number">1.1.5.</span> <span class="toc-text">回归与分类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归"><span class="toc-number">1.2.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练与损失"><span class="toc-number">1.3.</span> <span class="toc-text">训练与损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#降低损失-Reducing-Loss"><span class="toc-number">1.4.</span> <span class="toc-text">降低损失(Reducing Loss)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用-TensorFlow-的起始步骤"><span class="toc-number">1.5.</span> <span class="toc-text">使用 TensorFlow 的起始步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#过拟合和泛化"><span class="toc-number">1.6.</span> <span class="toc-text">过拟合和泛化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拆分数据"><span class="toc-number">1.7.</span> <span class="toc-text">拆分数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征工程"><span class="toc-number">1.8.</span> <span class="toc-text">特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#过程"><span class="toc-number">1.8.1.</span> <span class="toc-text">过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特点"><span class="toc-number">1.8.2.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据清理"><span class="toc-number">1.8.3.</span> <span class="toc-text">数据清理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征组合-Feature-Crosses"><span class="toc-number">1.9.</span> <span class="toc-text">特征组合(Feature Crosses)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#对非线性规律进行编码"><span class="toc-number">1.9.1.</span> <span class="toc-text">对非线性规律进行编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#组合独热矢量"><span class="toc-number">1.9.2.</span> <span class="toc-text">组合独热矢量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#简化正则化-Regularization-for-Simplicity-：L₂-正则化"><span class="toc-number">1.10.</span> <span class="toc-text">简化正则化 (Regularization for Simplicity)：L₂ 正则化</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://blog-1252404748.cos.ap-chengdu.myqcloud.com/avatar.jpg"></div><div class="author-info__name text-center">Ykk</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/cloisonne" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">43</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">34</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://blog-1252404748.cos.ap-chengdu.myqcloud.com/bg-1.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Ykk</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/about">About</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">google机器学习速成课程总结</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-03-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Notes/">Notes</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,798</span><span class="post-meta__separator">|</span><span>Reading time: 12 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><p>Google新出的16小时机器学习速成课程，主要针对TensorFlow，记一些重点。主题不支持MathJax，公式省略。</p>
<a id="more"></a>
<hr>
<h2 id="机器学习概念"><a href="#机器学习概念" class="headerlink" title="机器学习概念"></a>机器学习概念</h2><hr>
<h3 id="机器学习主要术语"><a href="#机器学习主要术语" class="headerlink" title="机器学习主要术语"></a>机器学习主要术语</h3><h4 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h4><p>在简单线性回归中，标签是我们要预测的事物，即 y 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。</p>
<h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><p>在简单线性回归中，特征是输入变量，即 x 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：</p>
<p>在垃圾邮件检测器示例中，特征可能包括：</p>
<ul>
<li>电子邮件文本中的字词</li>
<li>发件人的地址</li>
<li>发送电子邮件的时段</li>
<li>电子邮件中包含“一种奇怪的把戏”这样的短语。</li>
</ul>
<h4 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h4><p>样本是指数据的特定实例：x。（我们采用粗体 x 表示它是一个矢量。）我们将样本分为以下两类：</p>
<ul>
<li>有标签样本</li>
<li>无标签样本</li>
</ul>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>模型定义了特征与标签之间的关系。例如，垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。我们来重点介绍一下模型生命周期的两个阶段：</p>
<ul>
<li><p>训练表示创建或学习模型。也就是说，您向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。</p>
</li>
<li><p>推断表示将训练后的模型应用于无标签样本。也就是说，您使用训练后的模型来做出有用的预测 (y’)。</p>
</li>
</ul>
<h4 id="回归与分类"><a href="#回归与分类" class="headerlink" title="回归与分类"></a>回归与分类</h4><ul>
<li><p>回归模型可预测连续值。例如，回归模型做出的预测可回答如下问题：</p>
</li>
<li><p>分类模型可预测离散值。例如，分类模型做出的预测可回答如下问题：</p>
</li>
</ul>
<hr>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>过</p>
<hr>
<h3 id="训练与损失"><a href="#训练与损失" class="headerlink" title="训练与损失"></a>训练与损失</h3><p>损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。</p>
<ul>
<li><p>平方损失（又称为 L2 损失）</p>
</li>
<li><p>均方误差 (MSE)</p>
<p>  求出各个样本的所有平方损失之和，然后除以样本数量</p>
</li>
</ul>
<hr>
<h3 id="降低损失-Reducing-Loss"><a href="#降低损失-Reducing-Loss" class="headerlink" title="降低损失(Reducing Loss)"></a>降低损失(Reducing Loss)</h3><ul>
<li><p>迭代方法</p>
</li>
<li><p>梯度下降法</p>
</li>
<li><p>学习速率</p>
</li>
<li><p>优化学习速率</p>
</li>
<li><p>随机梯度下降法（SGD）</p>
<p>  小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。</p>
</li>
</ul>
<hr>
<h3 id="使用-TensorFlow-的起始步骤"><a href="#使用-TensorFlow-的起始步骤" class="headerlink" title="使用 TensorFlow 的起始步骤"></a>使用 TensorFlow 的起始步骤</h3><table>
<thead>
<tr>
<th>工具包</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Estimator (tf.estimator)</td>
<td>高级 OOP API</td>
</tr>
<tr>
<td>tf.layers/tf.losses/tf.metrics</td>
<td>用于常见模型组件的库</td>
</tr>
<tr>
<td>TensorFlow</td>
<td>低级 API</td>
</tr>
</tbody>
</table>
<p>TensorFlow 由以下两个组件组成：</p>
<ul>
<li>图协议缓冲区（Protocol Buffers）</li>
<li>执行（分布式）图的运行时</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up a linear classifier.</span></span><br><span class="line">classifier = tf.estimator.LinearClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model on some example data.</span></span><br><span class="line">classifier.train(input_fn=train_input_fn, steps=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use it to predict.</span></span><br><span class="line">predictions = classifier.predict(input_fn=predict_input_fn)</span><br></pre></td></tr></table></figure>
<p>一个例子：<br>    在 TensorFlow 中使用 LinearRegressor 类并基于单个输入特征预测各城市街区的房屋价值中位数<br>    使用均方根误差 (RMSE) 评估模型预测的准确率<br>    通过调整模型的超参数提高模型准确率</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://storage.googleapis.com/ml_universities/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机化处理</span></span><br><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br><span class="line">california_housing_dataframe[<span class="string">"median_house_value"</span>] /= <span class="number">1000.0</span></span><br><span class="line">california_housing_dataframe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查数据</span></span><br><span class="line">california_housing_dataframe.describe()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the input feature: total_rooms.</span></span><br><span class="line">my_feature = california_housing_dataframe[[<span class="string">"total_rooms"</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a numeric feature column for total_rooms.</span></span><br><span class="line">feature_columns = [tf.feature_column.numeric_column(<span class="string">"total_rooms"</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the label.</span></span><br><span class="line">targets = california_housing_dataframe[<span class="string">"median_house_value"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use gradient descent as the optimizer for training the model.</span></span><br><span class="line">my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.0000001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度裁剪 (gradient clipping)</span></span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure the linear regression model with our feature columns and optimizer.</span></span><br><span class="line"><span class="comment"># Set a learning rate of 0.0000001 for Gradient Descent.</span></span><br><span class="line">linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">    feature_columns=feature_columns,</span><br><span class="line">    optimizer=my_optimizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a linear regression model of one feature.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame of features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame of targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. Whether to shuffle the data.</span></span><br><span class="line"><span class="string">      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Convert pandas data into a dict of np arrays.</span></span><br><span class="line">    features = &#123;key:np.array(value) <span class="keyword">for</span> key,value <span class="keyword">in</span> dict(features).items()&#125;                                           </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Construct a dataset, and configure batching/repeating</span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features,targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle the data, if specified</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(buffer_size=<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line">	</span><br><span class="line">_ = linear_regressor.train(</span><br><span class="line">    input_fn = <span class="keyword">lambda</span>:my_input_fn(my_feature, targets),</span><br><span class="line">    steps=<span class="number">100</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an input function for predictions.</span></span><br><span class="line"><span class="comment"># Note: Since we're making just one prediction for each example, we don't </span></span><br><span class="line"><span class="comment"># need to repeat or shuffle the data here.</span></span><br><span class="line">prediction_input_fn =<span class="keyword">lambda</span>: my_input_fn(my_feature, targets, num_epochs=<span class="number">1</span>, shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call predict() on the linear_regressor to make predictions.</span></span><br><span class="line">predictions = linear_regressor.predict(input_fn=prediction_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Format predictions as a NumPy array, so we can calculate error metrics.</span></span><br><span class="line">predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> predictions])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print Mean Squared Error and Root Mean Squared Error.</span></span><br><span class="line">mean_squared_error = metrics.mean_squared_error(predictions, targets)</span><br><span class="line">root_mean_squared_error = math.sqrt(mean_squared_error)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Mean Squared Error (on training data): %0.3f"</span> % mean_squared_error</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Root Mean Squared Error (on training data): %0.3f"</span> % root_mean_squared_error</span><br></pre></td></tr></table></figure>
<p>写成一个函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(learning_rate, steps, batch_size, input_feature=<span class="string">"total_rooms"</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear regression model of one feature.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    input_feature: A `string` specifying a column from `california_housing_dataframe`</span></span><br><span class="line"><span class="string">      to use as input feature.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  </span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line"></span><br><span class="line">  my_feature = input_feature</span><br><span class="line">  my_feature_data = california_housing_dataframe[[my_feature]]</span><br><span class="line">  my_label = <span class="string">"median_house_value"</span></span><br><span class="line">  targets = california_housing_dataframe[my_label]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create feature columns</span></span><br><span class="line">  feature_columns = [tf.feature_column.numeric_column(my_feature)]</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create input functions</span></span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>:my_input_fn(my_feature_data, targets, batch_size=batch_size)</span><br><span class="line">  prediction_input_fn = <span class="keyword">lambda</span>: my_input_fn(my_feature_data, targets, num_epochs=<span class="number">1</span>, shuffle=<span class="keyword">False</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create a linear regressor object.</span></span><br><span class="line">  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">      feature_columns=feature_columns,</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Set up to plot the state of our model's line each period.</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">15</span>, <span class="number">6</span>))</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">  plt.title(<span class="string">"Learned Line by Period"</span>)</span><br><span class="line">  plt.ylabel(my_label)</span><br><span class="line">  plt.xlabel(my_feature)</span><br><span class="line">  sample = california_housing_dataframe.sample(n=<span class="number">300</span>)</span><br><span class="line">  plt.scatter(sample[my_feature], sample[my_label])</span><br><span class="line">  colors = [cm.coolwarm(x) <span class="keyword">for</span> x <span class="keyword">in</span> np.linspace(<span class="number">-1</span>, <span class="number">1</span>, periods)]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  <span class="keyword">print</span> <span class="string">"Training model..."</span></span><br><span class="line">  <span class="keyword">print</span> <span class="string">"RMSE (on training data):"</span></span><br><span class="line">  root_mean_squared_errors = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    linear_regressor.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">    predictions = linear_regressor.predict(input_fn=prediction_input_fn)</span><br><span class="line">    predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> predictions])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute loss.</span></span><br><span class="line">    root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(predictions, targets))</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"  period %02d : %0.2f"</span> % (period, root_mean_squared_error)</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    root_mean_squared_errors.append(root_mean_squared_error)</span><br><span class="line">    <span class="comment"># Finally, track the weights and biases over time.</span></span><br><span class="line">    <span class="comment"># Apply some math to ensure that the data and line are plotted neatly.</span></span><br><span class="line">    y_extents = np.array([<span class="number">0</span>, sample[my_label].max()])</span><br><span class="line">    </span><br><span class="line">    weight = linear_regressor.get_variable_value(<span class="string">'linear/linear_model/%s/weights'</span> % input_feature)[<span class="number">0</span>]</span><br><span class="line">    bias = linear_regressor.get_variable_value(<span class="string">'linear/linear_model/bias_weights'</span>)</span><br><span class="line"></span><br><span class="line">    x_extents = (y_extents - bias) / weight</span><br><span class="line">    x_extents = np.maximum(np.minimum(x_extents,</span><br><span class="line">                                      sample[my_feature].max()),</span><br><span class="line">                           sample[my_feature].min())</span><br><span class="line">    y_extents = weight * x_extents + bias</span><br><span class="line">    plt.plot(x_extents, y_extents, color=colors[period]) </span><br><span class="line">  <span class="keyword">print</span> <span class="string">"Model training finished."</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">  plt.ylabel(<span class="string">'RMSE'</span>)</span><br><span class="line">  plt.xlabel(<span class="string">'Periods'</span>)</span><br><span class="line">  plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(root_mean_squared_errors)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Output a table with calibration data.</span></span><br><span class="line">  calibration_data = pd.DataFrame()</span><br><span class="line">  calibration_data[<span class="string">"predictions"</span>] = pd.Series(predictions)</span><br><span class="line">  calibration_data[<span class="string">"targets"</span>] = pd.Series(targets)</span><br><span class="line">  display.display(calibration_data.describe())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">print</span> <span class="string">"Final RMSE (on training data): %0.2f"</span> % root_mean_squared_error</span><br></pre></td></tr></table></figure>
<p><strong>常用超参数</strong></p>
<ul>
<li>steps：是指训练迭代的总次数。一步计算一批样本产生的损失，然后使用该值修改模型的权重一次。</li>
<li>batch size：是指单步的样本数量（随机选择）。例如，SGD 的批量大小为 1。</li>
</ul>
<p><strong>方便变量</strong></p>
<pre><code>periods：控制报告的粒度。例如，如果 periods 设为 7 且 steps 设为 70，则练习将每 10 步（或 7 次）输出一次损失值。与超参数不同，我们不希望您修改 periods 的值。请注意，修改 periods 不会更改您的模型所学习的内容。
</code></pre><p><strong>合成特征和离群值</strong></p>
<ul>
<li>合成特征</li>
<li>识别离群值</li>
<li>截取离群值<br>  离群值设置为相对合理的最小值或最大值来进一步改进模型拟合情况。</li>
</ul>
<hr>
<h3 id="过拟合和泛化"><a href="#过拟合和泛化" class="headerlink" title="过拟合和泛化"></a>过拟合和泛化</h3><p>以下三项基本假设阐明了泛化：</p>
<ul>
<li>我们从分布中随机抽取独立同分布 (i.i.d) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。</li>
<li>分布是平稳的；即分布在数据集内不会发生变化。</li>
<li>我们从同一分布的数据划分中抽取样本。</li>
</ul>
<hr>
<h3 id="拆分数据"><a href="#拆分数据" class="headerlink" title="拆分数据"></a>拆分数据</h3><ul>
<li>训练集 - 用于训练模型的子集。</li>
<li>测试集 - 用于测试训练后模型的子集。</li>
</ul>
<hr>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><ul>
<li>映射数值</li>
<li>映射字符串（onehot encoding</li>
<li>映射分类（枚举）值</li>
</ul>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>避免很少使用的离散特征值</li>
<li>最好具有清晰明确的含义</li>
<li>不要将“神奇”的值与实际数据混为一谈（异常值</li>
<li>考虑上游不稳定性<h4 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h4></li>
</ul>
<ol>
<li>缩放特征值<ul>
<li>帮助梯度下降法更快速地收敛。</li>
<li>帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。</li>
<li>帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。</li>
</ul>
</li>
<li>处理极端离群值<ul>
<li>对数缩放</li>
<li>限制特征值</li>
</ul>
</li>
<li>分箱（划分范围</li>
<li>清查</li>
</ol>
<hr>
<h3 id="特征组合-Feature-Crosses"><a href="#特征组合-Feature-Crosses" class="headerlink" title="特征组合(Feature Crosses)"></a>特征组合(Feature Crosses)</h3><h4 id="对非线性规律进行编码"><a href="#对非线性规律进行编码" class="headerlink" title="对非线性规律进行编码"></a>对非线性规律进行编码</h4><p>特征组合是指通过将两个或多个输入特征相乘来对特征空间中的非线性规律进行编码的合成特征。“cross”（组合）这一术语来自 cross product（向量积）。</p>
<h4 id="组合独热矢量"><a href="#组合独热矢量" class="headerlink" title="组合独热矢量"></a>组合独热矢量</h4><p>对独热编码进行特征组合，则会得到可解读为逻辑连接的二元特征</p>
<p>线性学习器可以很好地扩展到大量数据。对大规模数据集使用特征组合是学习高度复杂模型的一种有效策略。</p>
<hr>
<h3 id="简化正则化-Regularization-for-Simplicity-：L₂-正则化"><a href="#简化正则化-Regularization-for-Simplicity-：L₂-正则化" class="headerlink" title="简化正则化 (Regularization for Simplicity)：L₂ 正则化"></a>简化正则化 (Regularization for Simplicity)：L₂ 正则化</h3><p>通过降低复杂模型的复杂度来防止过拟合，这种原则称为正则化。<br>以最小化损失和复杂度为目标，这称为<code>结构风险最小化</code>.</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ykk</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://ykksmile.top/posts/61970/">https://ykksmile.top/posts/61970/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/MachineLearning/">MachineLearning</a><a class="post-meta__tags" href="/tags/TensorFlow/">TensorFlow</a></div><div class="social-share" data-disabled="google,facebook,twitter"></div><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/posts/55073/"><i class="fa fa-chevron-left">  </i><span>面试准备阅读列表</span></a></div><div class="next-post pull-right"><a href="/posts/29034/"><span>关于Python元类metaclass和在ORM中的应用</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitment-container"></div><script>var gitment = new Gitment({
  owner: 'cloisonne',
  repo: 'cloisonne.github.io',
  oauth: {
    client_id: 'd6cdb36f97d08963e9cc',
    client_secret: '32f854846bf95d283ee654337570bbed41f69f03'
  }
})
gitment.render('gitment-container')</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2016 - 2018 By Ykk<span>   </span><i class="fa fa-heart animated infinite pulse"></i></div><div class="framework-info"><span>Hosted by   </span><a href="https://pages.coding.me"><span>Coding Pages </span></a><!--span.footer-separator |--><!--i(class='fa fa-heart animated infinite pulse')--><!--span= _p('footer.theme') + ' - '--><span> &amp; </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.4"></script><script src="/js/fancybox.js?version=1.5.4"></script><script src="/js/sidebar.js?version=1.5.4"></script><script src="/js/copy.js?version=1.5.4"></script><script src="/js/fireworks.js?version=1.5.4"></script><script src="/js/transition.js?version=1.5.4"></script><script src="/js/scroll.js?version=1.5.4"></script><script src="/js/head.js?version=1.5.4"></script></body></html>